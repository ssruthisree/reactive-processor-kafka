Reproducing kafka producer error that causing the stream to break

- Run docker compose up to start kafka
- Messages should be sourced to topic teststock
- aggregate function consumes the messages
- "bar" messages are processed and sent to destination topic transform.
- "foo" messages result in runtime exception, which is caught by onErrorContinue and handled. Stream does not break as no exception is thrown out of this block.
- Delete topic "transform". The topic does not get recreated as in the docker-compose file auto create of topics is explicitly set to false (KAFKA_AUTO_CREATE_TOPICS_ENABLE)
- Now should start seeing below logs,
   [kafka-producer-network-thread | producer-4] NetworkClient : [Producer clientId=producer-4] Error while fetching metadata with correlation id 11 : {transform=UNKNOWN_TOPIC_OR_PARTITION}
- After 60 seconds (default retry executed) producer exception is thrown.
  Caused by: org.springframework.messaging.MessageHandlingException: error occurred in message handler [org.springframework.cloud.stream.binder.kafka.KafkaMessageChannelBinder$ProducerConfigurationMessageHandler@2c6d3a3f]; nested exception is org.springframework.kafka.KafkaException: Send failed; nested exception is org.apache.kafka.common.errors.TimeoutException: Topic transform not present in metadata after 60000 ms.
- After which the stream breaks stating no subscribers available,
  Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers, failedMessage=GenericMessage [payload=byte[3], headers={kafka_offset=17, scst_nativeHeadersPresent=true, kafka_consumer=org.apache.kafka.clients.consumer.KafkaConsumer@1e903d7b, deliveryAttempt=3, kafka_timestampType=CREATE_TIME, kafka_receivedPartitionId=0, contentType=application/json, kafka_receivedTopic=testtock, kafka_receivedTimestamp=1639768679700, kafka_groupId=anonymous.d57fb6f0-5592-4a5c-a9b6-df49f709fe37}]



Note: We are able to get hold of the exception thrown in Hooks.onErrorDropped. But this does not help as once the exception is thrown, the dispatcher no more has a subscriber and even if the topic is recreated, the messages are not consumed by the aggregate function.
Unless application is restarted, we are not able to recover from this.



Added a generated log file into the project as well for reference: log_with_stream_break.log
